{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝의 이론과 실제\n",
    "\n",
    "## 1. 텍스트 마이닝의 이해\n",
    "\n",
    "### 1-1. 텍스트 마이닝이란?\n",
    "\n",
    "* 위키피디아 정의: the process of deriving high-quality information from text. (텍스트로 부터 양질의 정보를 뽑아내는 과정)  \n",
    "텍스트 안의 패턴, 경향에서 정보 파악 (by statistical pattern learning)  \n",
    "NLP(자연어 처리 과정) 과 분석 방법을 통해 **비정형 데이터 -> 정형 데이터**  \n",
    "  \n",
    "  \n",
    "* Turn unstructured text into structured data (일정한 길이의 vector로 변환 - 숫자)  \n",
    "머신러닝으로 다룰 수 있게 vector로 변환한 후 **머신러닝 (딥러닝) 기법**에 적용\n",
    " * text classification, clustering, sentiment analysis, document summarization, translation, prediction, etc.\n",
    "  \n",
    "ㅡ> 자연어 처리, 통계학 & 선형대수, 머신러닝, 딥러닝에 대한 기본 지식 요구됨  \n",
    "  \n",
    "  \n",
    "## 2. 텍스트 마이닝 방법론\n",
    "\n",
    "* NLP (Natural Laguage Processing)\n",
    " *Tokenize, stemming, lemmatize\n",
    " * Chunking\n",
    " * BOW, TFIDF – sparse representation\n",
    "* 머신러닝 (딥러닝)\n",
    " * Naïve Bayes, Logistic egression, Decision tree, SVM\n",
    " * Embedding(Word2Vec, Doc2Vec) – dense representation\n",
    " * RNN(LSTM), Attention, Transformer\n",
    "  \n",
    "  \n",
    "* 텍스트 마이닝 단계  \n",
    "Document -(tokenize 단어단위로 쪼개기 / normalize 표준화)-> sequence of normalized words  \n",
    " * -> Fixed size vector without sequence info. (순서 무시) - Naïve Bayes, Decision Tree, SVM, Logistic Regression, MLP  \n",
    " * -> Fixed size vector with sequence info. (순서=문맥 정보 포함) - Decision Tree, SVM, Logistic Regression, MLP  \n",
    " * -> Series of Word Embedding with sequence info. - RNN, Bi-LSTM, Transformer, BERT  \n",
    "  \n",
    "  \n",
    "* 적용 분야  \n",
    " * Document classification (Sentiment analysis, classification)  \n",
    " * Document generation (Q&A, summarization, translation - 지금은 각각 분야로 커짐 )  \n",
    " * Keyword extraction (tagging/annotation)  \n",
    " * Topic modeling (LSA(Latent Semantic Analysis), LDA(Latent Dirichlet Allocation)    \n",
    "\n",
    "### 2-1. 도구 및 원리의 이해\n",
    "\n",
    "* 주로 파이썬 사용\n",
    " * NLTK\n",
    " * Scikit Learn\n",
    " * Gensim\n",
    " * Keras\n",
    "  \n",
    "  \n",
    "* 텍스트 마이닝 기본 도구 (NLP 중심)  \n",
    "목적: document, sentence 등을 머신러닝에 사용 가능한 sparse (0이 많은) vector로 변환\n",
    " * **Tokenize**: 대상이 되는 문서/문장을 최소 단위로 쪼갬\n",
    " * **Text normalization**: 최소 단위를 표준화\n",
    " * **POS-tagging**: 최소 의미단위로 나누어진 대상에 대해 품사를 부착\n",
    " * **Chunking**: POS-tagging의 결과를 명사구, 형용사구, 분사구 등과 같은 말모듬으로 다시 합치는 과정\n",
    " * **BOW, TFIDF**: tokenized 결과를 이용하여 문서를 vector로 표현\n",
    " \n",
    "#### Tokenize\n",
    "Document를 Sentence의 집합/ Sentence를 Word의 집합으로 분리  \n",
    "의미 없는 문자 등을 걸러 냄  \n",
    "  \n",
    "  \n",
    "영어는 공백(space) 기준으로 비교적 쉽게 tokenize 가능  \n",
    "but 한글은 구조상 형태소(morpheme) 분석이 필요 -> 복합명사, 조사, 어미 등을 분리해내는 작업이 필요 (영어에 비해 어렵고 정확도 낮음)\n",
    "\n",
    "#### Text Normalization\n",
    "동일한 의미의 단어가 다른 형태를 갖는 것을 보완\n",
    "* **Stemming (어간 추출)**  \n",
    "의미가 아닌 규칙(알고리즘)에 의한 변환  \n",
    "(단수 – 복수, 현재형 – 미래형 등 단어의 다양한 변형을 하나로 통일 - 문제 없음)  \n",
    "* **Lemmatization (표제어 추출)**  \n",
    "사전을 이용하여 단어의 원형을 추출 (의미 파악)  \n",
    "품사(part-of-speech)를 고려\n",
    "\n",
    "#### POS-tagging\n",
    "토큰화와 정규화 작업을 통해 나누어진 형태소(의미를 가지는 최소단위)에 대해 품사를 결정하여 할당하는 작업  \n",
    "동일한 단어라도 문맥에 따라 의미가 달라지므로 품사를 알기 위해서는 문맥을 파악해야 함  \n",
    "-> Text-to-speech에서 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "Chunk는 언어학적으로 말모듬을 뜻하며, 명사구, 형용사구, 분사구 등과 같이 주어와 동사가 없는 두 단어 이상의 집합인 구(phrase)를 의미  \n",
    "Chunking은 주어진 텍스트에서 이와 같은 chunk를 찾는 과정  \n",
    "(= 형태소 분석의 결과인 각 형태소들을 서로 겹치지 않으면서 의미가 있는 구로 묶어나가는 과정)  \n",
    "텍스트로부터 Information Extraction(정보추출)을 하기 위한 전단계로 보거나 혹은 Information Extraction에 포함되기도 함\n",
    "\n",
    "#### 개체명 인식(Named Entity Recognition, NER)\n",
    "개체명(Named Entity)은 기관, 단체, 사람, 날짜 등과 같이 특정\n",
    "정보에 해당하는 명사구를 의미  \n",
    "텍스트로부터 뭔가 의미 있는 정보를 추출하기 위한 방법   \n",
    "* 관계 인식 (Relation Extraction)\n",
    "NER에 의해 추출된 개체명들을 대상으로 그들 간의 관계를 추출하는 작업  \n",
    "특정 건물이 특정 장소에 위치하는 관계와 같은 지식을 텍스트로부터 추출할 때 사용   \n",
    "  \n",
    "#### BOW (Bag of Words)   \n",
    "* Vector Space Model\n",
    " * 문서를 bag of words 로 표현\n",
    " * 단어가 쓰여진 순서는 무시\n",
    " * 모든 문서에 한번 이상 나타난 단어들에 대해 유(1)/무(0) 로 문서를 표현\n",
    "* count vector\n",
    " * 단어의 유/무 대신 단어가 문서에 나타난 횟수로 표현\n",
    " * count가 weight로 작용\n",
    "  \n",
    "* 활용\n",
    " * Retrieving matching documents\n",
    " * Classification and prediction\n",
    "\n",
    "  \n",
    "  \n",
    "#### TFIDF(Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "* count vector의 문제점  \n",
    "많은 문서에 공통적으로 나타난 단어는 중요성이 떨어지는 단어일 가능성이 높음 ex) the, a, …\n",
    "  \n",
    "  \n",
    "* TFIDF - 다양한 변형이 있음  \n",
    "단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림\n",
    " * tf(d, t): 문서 d에 단어 t가 나타난 횟수, count vector와 동일, 로그스케일 등 다양한 변형이 있음  \n",
    " * df(t): 전체 문서 중에서 단어 t를 포함하는 문서의 수  \n",
    " * idf(t): df(t)의 역수를 바로 쓸 수도 있으나, 여러가지 이유로 로그스케일과 스무딩을 적용한 공식을 사용, log(n/(1+df(t)), n은 전체 문서 수 \n",
    "   \n",
    "   \n",
    "* tokenize 결과 예제   \n",
    " * d1: fast, furious, shoot, shoot\n",
    " * d2: fast, fast, fly, furious\n",
    "* count vector: tf(d, t)  \n",
    "* idf: log(n/(1+df(t)) - 출현 빈도에 따라 중요도 차이를 줌\n",
    "* TFIDF: tf(d, t) * idf(t) - 가중치 변화 확인  \n",
    "  \n",
    "  \n",
    "* 유사도 계산\n",
    " * TFIDF vector의 내적을 이용\n",
    "* Cosine Similarity\n",
    " * vector의 방향에 대한 유사도\n",
    "\n",
    "\n",
    "#### Text Classification with BOW/TFIDF\n",
    "* Naïve Bayes\n",
    "* Logistic regression\n",
    " * Ridge regression\n",
    " * Lasso regression\n",
    "* Decision tree\n",
    " * Random Forest\n",
    "\n",
    "#### Naïve Bayes\n",
    "Wikipedia: a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies (count vector) as the features.\n",
    "* (x1,… xn) 의 단어집합으로 이루어진 문서가 분류 Ck(스팸일 확률)에 속할 확률\n",
    "\n",
    "#### Logistic Regression\n",
    "* 분류를 위한 회귀분석\n",
    " * 종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용\n",
    " * 종속 변수가 범주형 데이터를 대상으로 하며, 입력 데이터가 주어졌을 때 해당 데이터의 결과가 특정 분류로 나뉘기 때문에 일종의 분류 (classification) 기법에 해당  \n",
    "  \n",
    "  \n",
    "* 텍스트 마이닝에서의 문제\n",
    " * 추정해야 할 계수가 vector의 크기(단어의 수)만큼 존재하므로, 과적합이 발생하기 쉽고 많은 데이터 셋이 필요\n",
    " * 그럼에도 불구하고 잘 작동하는 편 - 정규화(regulation)을 이용해 과적합 해결 노력\n",
    "\n",
    "#### Ridge and Lasso Regression\n",
    "* 릿지 회귀 (Ridge regression)\n",
    " * 목적함수에 추정할 계수(parameter)에 대한 L2 norm(규제항)을 추가하여 모형의 과적합을 방지  \n",
    "  \n",
    "  \n",
    "* 라쏘 회귀 (Lasso regression)\n",
    " * L1 norm을 규제항으로 사용함으로써 0에 가까운 계수를 0으로 만들어 영향을 거의 미치지 않는 단어들을 제외\n",
    " * 남은 단어들로 분류의 이유에 대해 설명이 가능하다는 장점이 있음\n",
    " * feature selection의 효과가 있음\n",
    "\n",
    "#### 문서분류의 활용 - Sentiment Analysis (감성분석)\n",
    "소비자의 감성과 관련된 텍스트 정보를 자동으로 추출하는 텍스트 마이닝(Text Mining) 기술의 한 영역.   \n",
    "문서를 작성한 사람의 감정을 추출해 내는 기술로 문서의 주제보다 어떠한 감정을 가지고 있는가를 판단하여 분석한다. \n",
    "* 네이버 지식백과\n",
    "* Wikipedia – 보다 포괄적\n",
    "\n",
    "#### 한글 감성분석 예제\n",
    "label이 0이면 부정, 1이면 긍정  \n",
    "-> 리뷰를 BOW로 변환 후 input으로 쓰고, label을 target으로 하여 학습  \n",
    "-> 나이브베이즈, 로지스틱 회귀분석, SVM 등 다양한 방법의 사용이 가능  \n",
    "-> 새로운 리뷰에 대해 긍정/부정을 예측\n",
    "  \n",
    "  \n",
    "## 3. 텍스트 마이닝의 문제점\n",
    "### 3-1. 텍스트 마이닝의 문제\n",
    "\n",
    "#### Curse of Dimensionality\n",
    "\n",
    "* 차원의 저주  \n",
    "Extremely sparse data (각 데이터 간의 거리가 너무 멀게 위치)\n",
    "  \n",
    "  \n",
    "* 해결방법\n",
    " * 더 많은 데이터\n",
    " * Dimension reduction (차원 축소)  \n",
    "(feature selection, feature extraction)\n",
    "\n",
    "#### 단어 빈도의 불균형\n",
    "\n",
    "* Zipf’s law(멱법칙)\n",
    "극히 소수의 데이터가 결정적인 영향을 미치게 됨 (빈도가 극단적)\n",
    "  \n",
    "  \n",
    "* 해결방법\n",
    " * feature selection  \n",
    " 빈도 높은 단어를 삭제 (심한 경우 50% 삭제)\n",
    " * Boolean BOW 사용  \n",
    " 1이상이면 1로 변환\n",
    " * log 등의 함수를 이용해 weight를 변경\n",
    "\n",
    "#### 단어가 쓰인 순서정보의 손실\n",
    "\n",
    "* 통계에 의한 의미 파악 vs. 순서에 의한 의미 파악\n",
    "Loss of sequence information (단어들의 순서 - context가 중요)\n",
    "  \n",
    "* 해결방법\n",
    " * feature selection\n",
    " * Boolean BOW 사용\n",
    " * log 등의 함수를 이용해 weight를 변경\n",
    "\n",
    "#### 단어가 쓰인 순서정보의 손실\n",
    "\n",
    "* 통계에 의한 의미 파악 vs. 순서에 의한 의미 파악\n",
    "극히 소수의 데이터가 결정적인 영향을 미치게 됨 (빈도가 극단적)\n",
    "  \n",
    "  \n",
    "* 해결방법\n",
    " * n-gram  \n",
    " 부분적 해결, 주로 classification 문제에서 유용\n",
    " * Deep learning (근본적 해결방안)  \n",
    " RNN, Attention, Transformer, BERT\n",
    "\n",
    "### 3-2. 텍스트 마이닝의 문제 해결 방안\n",
    "\n",
    "### Dimensionality Reduction (차원 축소)\n",
    "차원의 저주를 해결하기 위한 노력\n",
    "\n",
    "* Feature selection  \n",
    "Manual, Regularization(Lasso)  \n",
    "* Feature extraction  \n",
    "PCA, LSA(SVD)  \n",
    "* Embedding\n",
    "Word embedding, Document embedding  \n",
    "* Deep Learning  \n",
    "RBM, Autoencoder  \n",
    "\n",
    "#### Feature Extraction\n",
    "Wiki: feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant\n",
    "\n",
    "* PCA (주성분 분석)\n",
    "데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변환함으로써 차원을 축소  \n",
    "\n",
    "* LSA(Latent Semantic Analysis)  \n",
    "a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis)\n",
    " * SVD (특이값 분해 - Singular Vector Decomposition)  \n",
    " 하나의 벡터를 세개의 벡터의 곱으로 분리  \n",
    " -> 축소된 세개의 벡터를 복원해도 원래의 벡터값과 근사함 (잠재의미 분석)\n",
    " * 잠재의미분석의 활용  \n",
    " 문서 간의 유사도, 단어 간의 유사도\n",
    " \n",
    "#### Topic Modeling\n",
    "문서엔 토픽들이 많음  \n",
    "토픽은 단어들의 확률 분포  \n",
    "generative model - 토픽에 주어진 가중치에 따라 토픽으로부터의 단어들을 선택하여 문서 생성\n",
    "문서 모음을 생성을 담당하는 토픽의 집합을 유추  \n",
    "  \n",
    "  \n",
    "* 원리: 토픽은 주제를 의미하는 용어로 사용되며, 각 문서들이 특정한 주제에 속할 확률분포와 주제로부터 특정 단어들이 파생되어 나올 확률분포가 주어졌을 때, 이 두 확률분포를 조합하여 각 문서들에 들어가는 단어들의 확률분포를 계산  \n",
    "  \n",
    "  \n",
    "* 목적: 관찰된 데이터(문서의 단어들)를 설명할 수 있는 최적의 잠재 변수 집합 찾기  \n",
    "(word distribution과 topic distribution 모두 알아냄)\n",
    "  \n",
    "  \n",
    "* Determining the number of topics  \n",
    " -Too many topics: Uninterpretable topics  \n",
    " -Too few topics: Very broad topics\n",
    "  \n",
    "  \n",
    "* Latent Dirichlet Allocation  \n",
    " * α is the parameter of the Dirichlet prior on the per-document topic distributions  \n",
    "(각 문서의 토픽이 어떤 확률분포로 이뤄져 있는지에 대한 파라미터)  \n",
    " * β is the parameter of the Dirichlet prior on the per-topic word distribution  \n",
    " (한 토픽의 단어들이 어떤 확률 분포로 이뤄져 있는지에 대한 파라미터) \n",
    "  \n",
    "  \n",
    "* 활용사례\n",
    "드라마 시청률 변화 <-> 소셜 미디어 토픽 변화\n",
    "\n",
    "#### Word Embedding\n",
    "\n",
    "* 단어에 대한 vector의 dimension reduction이 목표\n",
    "* 단어의 표현\n",
    " * Term-Document Matrix에서 Document 별 count vector - 일반화가 어려움\n",
    " * one-hot-encoding: extremely sparse (하나만 1 나머지 0)\n",
    " 각 단어를 모든 문서에서 사용된 단어들의 수 길이의 벡터로 표현  \n",
    " (심한 경우 길이 30만의 벡터 중에서 하나만 1인 sparse vector가 됨)\n",
    "  \n",
    "  \n",
    "* Word embedding\n",
    " * one-hot-encoding -> dense vector\n",
    " * 변환된 vector를 이용하여 학습\n",
    " * 최종목적에 맞게 학습에 의해 vector가 결정됨\n",
    " * 학습목적 관점에서의 단어의 의미를 내포\n",
    "  \n",
    "  \n",
    "* BOW vs Word Embedding  \n",
    " * BOW - 단어의 표현에는 관심이 없음  \n",
    " * Word Embedding - 문맥(순서) 중시  \n",
    "  \n",
    "  \n",
    "* Word Embedding을 이용한 문서 분류  \n",
    " * BOW와는 다른 관점의 문서 표현  \n",
    "-> document: 제한된 maxlen 개의 word sequence (앞이나 뒤를 잘라냄)  \n",
    "(maxlen, reduced_dim)의 2차원 행렬로 표현  \n",
    "-> word: one-hot-vector에서 저차원(reduced_dim)으로 embedding된 dense vector  \n",
    " * 단순한 분류모형 만들려면 (sequence 무시한)  \n",
    "(maxlen, reduced_dim) 차원의 document를 maxlen*reduced_dim 차원으로 펼쳐서 분류모형에 적용\n",
    "\n",
    "#### Word2Vec\n",
    "문장에 나타난 단어들의 순서를 이용해 (학습 목적에 맞춰) word embedding을 수행\n",
    "* CBOW: 주변단어들을 이용해 다음 단어를 예측\n",
    "* Skip-gram: 한 단어의 주변단어들을 예측\n",
    "  \n",
    "  \n",
    "* 학습 방법\n",
    " * sliding window를 이용한 학습 set 구성  \n",
    " 주어진 주변 단어들을 입력했을때, target word의 확률이 높아지도록 학습 혹은 그 반대\n",
    " * embedding vector  \n",
    " nput이 one-hot vector이므로 W가 embedding vector의 집합이 됨\n",
    "  \n",
    "  \n",
    "* 의미  \n",
    "단어의 위치에 기반하여 의미를 내포하는 vector 생성  \n",
    "-> 비슷한 위치에 나타나는 단어들은 비슷한 vector를 가지게 됨  \n",
    "-> 단어 간의 유사성을 이용하여 연산이 가능  \n",
    "\n",
    "#### ELMo (Embeddings from Language Model)\n",
    "사전 훈련된 언어 모델을 사용하는 워드 임베딩 방법론\n",
    "* 문맥 반영  \n",
    "이전의 대표적인 임베딩 기법인 Word2Vec이나 GloVe 등이 동일한 단어가 문맥에 따라 전혀 다른 의미를 가지는 것을 반영하지 못하지 못하는 것에 비해 ELMo는 이러한 문맥을반영하기 위해 개발된 워드 임베딩 기법\n",
    "* 문맥의 파악을 위해 biLSTM으로 학습된 모형을 이용\n",
    "  \n",
    "  \n",
    "#### Transfer Learning\n",
    "Wiki: storing knowledge gained while solving one problem\n",
    "and applying it to a different but related problem\n",
    "\n",
    "* 텍스트 마이닝에서의 전이학습\n",
    " * feature level: 단어에 대한 dense vector(word embedding)를 새로 학습하지 않고 학습된 vector를 그대로 가져다 씀\n",
    " * model level: word embedding과 모형 전체를 가져다 학습\n",
    " * Word2Vec, Glove, ELMo 등의 사전학습된 word embedding이 전이학습에 많이 사용됨\n",
    "\n",
    "#### Document Embedding\n",
    "\n",
    "* Word2Vec은 word에 대해 dense vector를 생성하지만, document vector는 여전히 sparse\n",
    "* Word2Vec 모형에서 주변단어들에 더하여 document의 고유한 vector를 함께 학습함으로써 document에 대한 dense vector를 생성  \n",
    "-> 이 dense vector를 이용해 매칭, 분류 등의 작업 수행\n",
    "  \n",
    "  \n",
    "#### RBM (Restricted Boltzmann Machine)g\n",
    "\n",
    "* 사전학습 목적으로 개발  \n",
    "-> 차원을 변경하면서 원래의 정보량 유지가 목적 (정보량을 물리학의 에너지 함수로 표현)  \n",
    "* Deep NN의 vanishing gradient 문제 해결을위해 제안  \n",
    "-> batch normalization, Dropout, ReLU 등의 기법으로 인해 문제가 해결되면서 지금은 많이 쓰이지 않음\n",
    "* 사전학습을 통한 차원 축소에 사용 가능\n",
    "  \n",
    "  \n",
    "#### Autoencoder\n",
    "\n",
    "* RBM과 유사한 개념  \n",
    "-> encoder로 차원을 축소하고 decoder로 다시 복원했을 때, 원래의 X와 복원한 X’이 최대한 동일하도록 학습\n",
    "* 작동방식은 PCA와 유사   \n",
    "-> 데이터에 내재된 일정한 구조 – 연관성을 추출\n",
    "  \n",
    "  \n",
    "#### Context(sequence)의 파악\n",
    "\n",
    "* N-gram\n",
    " * 문맥(context)를 파악하기 위한 전통적 방법\n",
    " * 대상이 되는 문자열을 하나의 단어 단위가 아닌, 두개 이상의 단위로 잘라서 처리 (bi-gram, tri-gram, …)  \n",
    " * 보통 unigram에 bi-gram, tri-gram을 추가하면서 feature의 수가 증가시켜 사용  \n",
    " -> 문맥 파악에 유리하나, dimension이 더욱 증가\n",
    "  \n",
    "  \n",
    "* 딥러닝 – RNN\n",
    " * 문장을 단어들의 sequence 혹은 series로 처리\n",
    " * 뒷 단어에 대한 hidden node가 앞 단어의 hidden node 값에도 영향을 받도록 함\n",
    " * 그 외에도 단어들 간의 관계를 학습할 수 있는 모형을 고안\n",
    " * Sequence 정보를 기억할 수 있는 방법은? \n",
    " -> hidden node가 mbedding된 word들을 순서대로 나열한  X 뿐만 아니라 이전 hidden node로부터도 입력을 받음\n",
    "  \n",
    "  \n",
    "* RNN의 문제:  \n",
    "문장이 길수록 층이 깊은 형태를 갖게 됨  \n",
    "-> 경사가 소실되는 문제 발생   \n",
    "-> 앞부분의 단어 정보가 학습되지 않음  \n",
    "\n",
    "#### LSTM (Long Short Term Memory)\n",
    "직통 통로를 만들어 RNN의 문제를 해결\n",
    "  \n",
    "  \n",
    "* 단방향 LSTM의 문제:  \n",
    "단어 순서가 갖는 문맥 정보가 한 방향으로만 학습된다.   \n",
    "자신의 뒤에 오는 단어에 의해 영향을 받는 경우, 학습이 되지 않음  \n",
    "  \n",
    "#### Bi-LSTM\n",
    "양방향으로 LSTM을 구성하여 두 결과를 합침  \n",
    "양방향 순서를 모두 학습\n",
    "  \n",
    "  \n",
    "#### 합성곱 신경망(Convolutional Neural Networks, CNN)\n",
    "* CNN은 원래 이미지 처리를 위해 개발된 신경망\n",
    "* 현재는 인간의 이미지 인식보다 더 나은 인식 성능을 보이고 있음.\n",
    "* CNN이 주변 정보를 학습한다는 점을 이용하여 텍스트의 문맥을 학습하여 문서를 분류하는 연구   \n",
    "-> 의외로 뛰어난 성능을 보이게 되면서 자연어 처리에서의 활용분야가 넓어지게 됨. \n",
    "* CNN은 합성곱층(conolution layer)와 풀링층(pooling)으 로 구성\n",
    " * 합성곱층은 2차원 이미지에서 특정 영역의 특징을 추출하는 역할을 하는데, 이는 연속된 단어들의 특징을 추출하는 것과 유사한 특성이 있음.\n",
    "  \n",
    "  \n",
    "* 구조\n",
    "합성곱층과 풀링층이 번갈아가면서 이미지의 특징을 단계적으로 추출하고, 마지막에 분류기를 통해 이미지를 판별하는 구조\n",
    "  \n",
    "  \n",
    "* CNN을 이용한 문서 분류\n",
    "이미지와 달리 텍스트는 단어들의 1차원 시퀀스로 표현되므로 1D CNN모형을 사용함.  \n",
    "단어 시퀀스에 대해 CNN의 필터는 1차원으로만 적용되고 이렇게 텍스트의 특징을 추출한 결과를 마찬가지로 분류기에 넣어서 문서를 판별.\n",
    "\n",
    "#### Sequence-to-sequence\n",
    "지금까지는 입력은 sequence, 출력은 하나의 값인 경우가 일반적\n",
    "but 번역, chat-bot, summarize등은 출력도 sequence가 되어야 함\n",
    "* encoder, decoder의 구조를 가짐\n",
    "\n",
    "#### Attention\n",
    "출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감한 것에 착안\n",
    "* 입력의 단어들로부터 출력 단어에 직접 링크를 만듦\n",
    "  \n",
    "#### Transformer (Self-attention)\n",
    "입력 단어들끼리도 상호연관성이 있는 것에 착안\n",
    "* 입력 -> 출력으로의 attention 외에 입력 단어들 간의 attention,\n",
    "* 입력 + 출력 -> 출력으로의 attention을 추가\n",
    "  \n",
    "  \n",
    "* encoder와 decoder가 서로 다른 attention 구조를 사용\n",
    " * multi-head attention vs masked multi-head attention\n",
    "* RNN이 사라지고 self-attention이 이를 대신\n",
    "\n",
    "#### BERT (Bidirectional Encoder Representations form Transformer)\n",
    "* 양방향 transformer 인코더를 사용\n",
    " * transformer에 기반한 OpenAI GPT와의 차이\n",
    "* transfer learning에서 feature + model을 함께 transfer하고 fine tuning을 통해서 적용하는 방식을 선택\n",
    "* 거의 모든 분야에서 top score를 기록\n",
    "  \n",
    "  \n",
    "* 참고\n",
    " * segment, position embedding을 사용\n",
    " * 다양한 text mining task에 전이학습을 이용해 적용 가능한 구조를 제안\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
